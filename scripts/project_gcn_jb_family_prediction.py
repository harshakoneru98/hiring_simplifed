# -*- coding: utf-8 -*-
"""Project GCN Jb family prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SVlvXYhcTNIiF8TK0kGyzv4Jkbe3WiqF
"""

!pip install torch
import torch
print(torch.__version__)

!pip install torch-geometric torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html

import torch_geometric.transforms as T
from torch_geometric.utils import degree
from torch_geometric.datasets import TUDataset

import pandas as pd
import numpy as np
from torch_geometric.data import Data
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader

# input_df = pd.read_csv("Graph_Classification_data.csv")
input_df = pd.read_csv("Graph_Classification_data_V3.csv")

input_df

# label_map =\
# {1:1, 2:2, 3:1,
#        4:3, 5:3,
#        6:4, 7:2}


# # jobs_data['rating'].unique()


# input_df['Label1'] = input_df['Label1'].apply(lambda x : label_map[x])

input_df

graph_ids = list(set(input_df['Graph_id'].values))

num_of_nodex = input_df['Node_id'].nunique()

num_classes = input_df['Label1'].nunique(0)

input_df['Label1'].unique()

num_of_nodex

# x_raw = [[0] for _ in range(num_of_nodex)]

# X1 = list(graph0['Node_x'].values)

# node_idx



# x_raw = [[0] for _ in range(num_of_nodex)]

# X1 = list(graph0['Node_x'].values)

# for id,val   in enumerate(node_idx):
#   print(id,val)
#   x_raw[val] = [X1[id]]

ll = [1,2,3]
#

def directed_edges(edge_lst):
  edged = []
  cnt = len(edge_lst)
  for i in range(len(edge_lst)):
    if i+1==cnt:
      continue
    else:
      edged.append([edge_lst[i],edge_lst[i+1]])
  edge_reversed = []
  for i in edged:
    edge_reversed.append([i[-1],i[0]])
  edged.extend(edge_reversed)


  return edged

# directed_edges(ll)

# ### directed graph with edges just between subsequent skills
# def generate_graph(id):
#   graph0 = input_df[input_df['Graph_id']==id]
#   node_idx = list(graph0['Node_id'].values)
#   x_raw = [[0] for _ in range(num_of_nodex)]
  
#   X1 = list(graph0['Node_x'].values)
#   for id,val   in enumerate(node_idx):
#   # print(id,val)
#     x_raw[val] = [X1[id]]


#   X_tmp = x_raw[:]


#   X = torch.tensor(X_tmp, dtype=torch.float)
  
#   # edge_list = []
#   # for i in node_idx:
#   #   for j in node_idx:
#   #     if i>=j:
#   #       continue
#   #     edge_list.append([i,j])
#   edge_list = directed_edges(node_idx)
#   edge_index_tmp = torch.tensor(edge_list, dtype=torch.long)
#   edge_index = edge_index_tmp.t().contiguous()

#   y = torch.tensor([graph0['Label1'].unique()[0]-1],dtype=torch.long)
#   # Data(x=x, edge_index=edge_index.t().contiguous())
#   data = Data(x = X ,edge_index = edge_index, y = y , num_nodes = num_of_nodex )
#   return data

######### fully connected graph 
def generate_graph(id):
  graph0 = input_df[input_df['Graph_id']==id]
  node_idx = list(graph0['Node_id'].values)
  x_raw = [[0,0] for _ in range(num_of_nodex)]
  
  X1 = list(graph0['Node_x'].values)
  X2 = list(graph0['Node_x2'].values)
  for id,val   in enumerate(node_idx):
  # print(id,val)
    x_raw[val] = [X1[id],X2[id]/3400]


  X_tmp = x_raw[:]


  X = torch.tensor(X_tmp, dtype=torch.float)
  
  edge_list = []
  for i in node_idx:
    for j in node_idx:
      if i==j:
        continue
      edge_list.append([i,j])
  edge_index_tmp = torch.tensor(edge_list, dtype=torch.long)
  edge_index = edge_index_tmp.t().contiguous()

  y = torch.tensor([graph0['Label1'].unique()[0]-1],dtype=torch.long)
  # Data(x=x, edge_index=edge_index.t().contiguous())
  data = Data(x = X ,edge_index = edge_index, y = y , num_nodes = num_of_nodex )
  return data

# def generate_graph(id):
#   x_raw = [[0] for _ in range(num_of_nodex)]
#   graph0 = input_df[input_df['Graph_id']==id]
#   X1 = list(graph0['Node_x'].values)

#   X_tmp = [[w] for w in X1]


#   X = torch.tensor(X_tmp, dtype=torch.float)
#   node_idx = list(graph0['Node_id'].values)
#   edge_list = []
#   for i in node_idx:
#     for j in node_idx:
#       if i==j:
#         continue
#       edge_list.append([i,j])
#   edge_index_tmp = torch.tensor(edge_list, dtype=torch.long)
#   edge_index = edge_index_tmp.t().contiguous()

#   y = torch.tensor([graph0['Label1'].unique()[0]-1],dtype=torch.long)
#   # Data(x=x, edge_index=edge_index.t().contiguous())
#   data = Data(x = X ,edge_index = edge_index, y = y , num_nodes = len(node_idx) )
#   return data

######### make list of data graphs

data_list = []

for id in graph_ids:
  data_list.append(generate_graph(id))

# data_list[0].x

### create dataloader object
import random
random.shuffle(data_list)
# loader = DataLoader(data_list, batch_size=32)

# for batch in loader:
#     print(batch)
#     print(batch.num_graphs)
#     break

data_list[0].edge_index

from torch_geometric.nn import aggr

# Simple aggregations:
mean_aggr = aggr.MeanAggregation()

num_of_nodex

from torch.nn import Linear
import torch.nn.functional as F
from torch_geometric.nn import GCNConv,GATConv
from torch_geometric.nn import global_mean_pool


class GCN(torch.nn.Module):
    def __init__(self, hidden_channels):
        super(GCN, self).__init__()
        #torch.manual_seed(12345)
        self.conv1 = GCNConv(2, hidden_channels)
        self.conv2 = GATConv(hidden_channels, int(hidden_channels/2))
        self.conv3 = GATConv(int(hidden_channels/2), int(hidden_channels/4))
        self.conv4 = GATConv(int(hidden_channels/4), int(hidden_channels/8))
        
        self.lin = Linear(int(hidden_channels/8), 7)

    def forward(self, x, edge_index, batch):
        # 1. Obtain node embeddings 
        x = self.conv1(x, edge_index)
        x = x.relu()
        x = self.conv2(x, edge_index)
        x = x.relu()
        x = self.conv3(x, edge_index)
        x = x.relu()    
        x = self.conv4(x, edge_index)
        # 2. Readout layer
        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]

        # 3. Apply a final classifier
        x = F.dropout(x, p=0.4, training=self.training)
        x = self.lin(x)
        
        return F.log_softmax(x, dim=1)

model = GCN(hidden_channels=1024)
print(model)#



train_loader = DataLoader(data_list[:3000], batch_size=32, shuffle=True)
test_loader = DataLoader(data_list[3000:], batch_size=32, shuffle=False)

def save_checkpoint(state, filename='./checkpoint.pth.tar'):
    """Save checkpoint if a new best is achieved"""
    if 1:
        print ("=> Saving a new best")
        torch.save(state, filename)  # save checkpoint
    else:
        print ("=> Validation Accuracy did not improve")

# for data in tarain_loader:
#   print(data)

# out.shape
#

# data.y.dtype = torch.float

# loss = criterion(torch.tensor([2],dtype=torch.float), torch.tensor([6],dtype=torch.float))

def get_default_device():
    """Picking GPU if available or else CPU"""
    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')
device = get_default_device()

device

from IPython.display import Javascript
display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))
# global out
model = GCN(hidden_channels=512).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = torch.nn.CrossEntropyLoss()

def train():
    # global out
    model.train()
    cnt = 0 
    for data in train_loader:
         data = data.to(device)
        # Iterate in batches over the training dataset.
        #  print(data.x, data.edge_index, data.batch)
         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.
         #print(out)
         #print(out)
         loss = criterion(out, data.y.to(device))  # Compute the loss.
         loss.backward()  # Derive gradients.
         optimizer.step()  # Update parameters based on gradients.
         optimizer.zero_grad()  # Clear gradients.

def test(loader):
     model.eval()

     correct = 0
     for data in loader:  # Iterate in batches over the training/test dataset.
         data = data.to(device)
         out = model(data.x, data.edge_index, data.batch)  
         pred = out.argmax(dim=1)  # Use the class with highest probability.
         correct += int((pred == data.y.to(device)).sum())  # Check against ground-truth labels.
     return correct / len(loader.dataset)  # Derive ratio of correct predictions.

def val(loader):
    model.eval()
    loss_all = 0
    for data in loader:
#        data = data.to(device)
        output = model(data.x, data.edge_index, data.batch)
        loss = F.nll_loss(output, data.y.view(-1))
        loss_all += loss.item() * data.num_graphs
    return loss_all / len(loader.dataset)

best_test_acc = 0
for epoch in range(1, 50):
    train()
    train_acc = test(train_loader)
    test_acc = test(test_loader)
    val_acc = 1
    
    val_loss = 1

    if test_acc > best_test_acc:
      best_test_acc = test_acc
      save_checkpoint(model.state_dict())

    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, Val Accuracy: {val_acc:.4f}, Val Loss: {val_loss:.4f}')

# for data in train_loader:
#   print(data)
#   print(data.edge_index)
#   print(data.x)
#   break



model2 = model.state_dict()

data = data_list[0]

live_loader = DataLoader(data_list[:], batch_size=1, shuffle=False)

model = GCN(hidden_channels=512).to(device)
checkpoint = torch.load('checkpoint.pth.tar')
model.load_state_dict(checkpoint)

pair = []
for data in live_loader:
  data = data.to(device)
  out = model(data.x, data.edge_index,data.batch)
  pred = out.argmax(dim=1)
  # print(int(pred))
  # print(int(data.y))
  pair.append((int(pred),int(data.y)))

from sklearn.metrics import confusion_matrix

y_actu = [w[1] for w in pair]
y_pred = [w[0] for w in pair]
confusion_matrix(y_actu, y_pred)





#ata_list = [data]

loader

